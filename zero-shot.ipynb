{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use zero shot classification models to make predictions on atrial fibrillation status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a zero shot classification classifier\n",
    "# here, we use the BART MNLI model, which is the most popular on the transformers library\n",
    "classifier = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       index  patient_id  afib  \\\n",
      "0          0         109     0   \n",
      "1          2         113     0   \n",
      "2          3         114     0   \n",
      "3          4         115     0   \n",
      "4          6         117     0   \n",
      "...      ...         ...   ...   \n",
      "29446  46515       97164     0   \n",
      "29447  46516       97484     0   \n",
      "29448  46517       97488     1   \n",
      "29449  46518       97492     0   \n",
      "29450  46519       97497     1   \n",
      "\n",
      "                                                   notes  age  \\\n",
      "0      PATIENT/TEST INFORMATION: Indication: Code. As...   25   \n",
      "1      Sinus rhythm, rate 93. Non-specific ST-T wave ...   35   \n",
      "2      Normal sinus rhythm, rate 96 Right bundle bran...   48   \n",
      "3      PATIENT/TEST INFORMATION: Indication: Left ven...   75   \n",
      "4      PATIENT/TEST INFORMATION: Indication: Murmur. ...   50   \n",
      "...                                                  ...  ...   \n",
      "29446  PATIENT/TEST INFORMATION: Indication: Aortic v...   83   \n",
      "29447  Sinus bradycardia with non-diagnostic repolari...   79   \n",
      "29448  PATIENT/TEST INFORMATION: Indication: Stroke  ...   67   \n",
      "29449  PATIENT/TEST INFORMATION: Indication: Cerebrov...   32   \n",
      "29450  PATIENT/TEST INFORMATION: Indication: Left ven...   58   \n",
      "\n",
      "                                             notes_half1  \\\n",
      "0      PATIENT/TEST INFORMATION: Indication: Code. As...   \n",
      "1      Sinus rhythm, rate 93. Non-specific ST-T wave ...   \n",
      "2      Normal sinus rhythm, rate 96 Right bundle bran...   \n",
      "3      PATIENT/TEST INFORMATION: Indication: Left ven...   \n",
      "4      PATIENT/TEST INFORMATION: Indication: Murmur. ...   \n",
      "...                                                  ...   \n",
      "29446  PATIENT/TEST INFORMATION: Indication: Aortic v...   \n",
      "29447  Sinus bradycardia with non-diagnostic repolari...   \n",
      "29448  PATIENT/TEST INFORMATION: Indication: Stroke  ...   \n",
      "29449  PATIENT/TEST INFORMATION: Indication: Cerebrov...   \n",
      "29450  PATIENT/TEST INFORMATION: Indication: Left ven...   \n",
      "\n",
      "                                             notes_half2  \n",
      "0      RICLE: RV hypertrophy.  PERICARDIUM: Small per...  \n",
      "1      ities. No previous tracing available for compa...  \n",
      "2      ge indeterminate Probable anterolateral subepi...  \n",
      "3       MR.  PERICARDIUM: No pericardial effusion.  G...  \n",
      "4      ned tricuspid valve leaflets. Moderate [2+] TR...  \n",
      "...                                                  ...  \n",
      "29446  Last Name (Titles) 2**] throughout the procedu...  \n",
      "29447  malities.  No previous tracing available for c...  \n",
      "29448  n. There is an anterior space which most likel...  \n",
      "29449  aflets with trivial MR.  TRICUSPID VALVE: Norm...  \n",
      "29450  CUSPID VALVE: Normal tricuspid valve leaflets ...  \n",
      "\n",
      "[29451 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# import the dataset\n",
    "data = pd.read_csv('processed_afib_data.csv')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m text \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mnotes\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m candidate_label \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39matrial fibrillation\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m output \u001b[39m=\u001b[39m classifier(text, candidate_label)\n\u001b[1;32m     11\u001b[0m \u001b[39m# extract the exact prediction from the output, which is a dictionary with the key 'scores' storing an array\u001b[39;00m\n\u001b[1;32m     12\u001b[0m prediction \u001b[39m=\u001b[39m output[\u001b[39m'\u001b[39m\u001b[39mscores\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/pipelines/zero_shot_classification.py:182\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline.__call__\u001b[0;34m(self, sequences, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to understand extra arguments \u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 182\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(sequences, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/pipelines/base.py:1074\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1073\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1074\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/pipelines/base.py:1096\u001b[0m, in \u001b[0;36mChunkPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1094\u001b[0m all_outputs \u001b[39m=\u001b[39m []\n\u001b[1;32m   1095\u001b[0m \u001b[39mfor\u001b[39;00m model_inputs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params):\n\u001b[0;32m-> 1096\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1097\u001b[0m     all_outputs\u001b[39m.\u001b[39mappend(model_outputs)\n\u001b[1;32m   1098\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(all_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/pipelines/base.py:990\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m    989\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 990\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m    991\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    992\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/pipelines/zero_shot_classification.py:201\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline._forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    199\u001b[0m sequence \u001b[39m=\u001b[39m inputs[\u001b[39m\"\u001b[39m\u001b[39msequence\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    200\u001b[0m model_inputs \u001b[39m=\u001b[39m {k: inputs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mmodel_input_names}\n\u001b[0;32m--> 201\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n\u001b[1;32m    203\u001b[0m model_outputs \u001b[39m=\u001b[39m {\n\u001b[1;32m    204\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcandidate_label\u001b[39m\u001b[39m\"\u001b[39m: candidate_label,\n\u001b[1;32m    205\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msequence\u001b[39m\u001b[39m\"\u001b[39m: sequence,\n\u001b[1;32m    206\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mis_last\u001b[39m\u001b[39m\"\u001b[39m: inputs[\u001b[39m\"\u001b[39m\u001b[39mis_last\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    207\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moutputs,\n\u001b[1;32m    208\u001b[0m }\n\u001b[1;32m    209\u001b[0m \u001b[39mreturn\u001b[39;00m model_outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1507\u001b[0m, in \u001b[0;36mBartForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[39mif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1503\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1504\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing input embeddings is currently not supported for \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1505\u001b[0m     )\n\u001b[0;32m-> 1507\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1508\u001b[0m     input_ids,\n\u001b[1;32m   1509\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1510\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1511\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1512\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1513\u001b[0m     decoder_head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1514\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1515\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[1;32m   1516\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1517\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1518\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1519\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1520\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1521\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1522\u001b[0m )\n\u001b[1;32m   1523\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]  \u001b[39m# last hidden state\u001b[39;00m\n\u001b[1;32m   1525\u001b[0m eos_mask \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39meq(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39meos_token_id)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1231\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1228\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1230\u001b[0m \u001b[39mif\u001b[39;00m encoder_outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1231\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1232\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1233\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1234\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1235\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1236\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1237\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1238\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[1;32m   1240\u001b[0m \u001b[39m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m \u001b[39melif\u001b[39;00m return_dict \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:850\u001b[0m, in \u001b[0;36mBartEncoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    843\u001b[0m         layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    844\u001b[0m             create_custom_forward(encoder_layer),\n\u001b[1;32m    845\u001b[0m             hidden_states,\n\u001b[1;32m    846\u001b[0m             attention_mask,\n\u001b[1;32m    847\u001b[0m             (head_mask[idx] \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    848\u001b[0m         )\n\u001b[1;32m    849\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 850\u001b[0m         layer_outputs \u001b[39m=\u001b[39m encoder_layer(\n\u001b[1;32m    851\u001b[0m             hidden_states,\n\u001b[1;32m    852\u001b[0m             attention_mask,\n\u001b[1;32m    853\u001b[0m             layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    854\u001b[0m             output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    855\u001b[0m         )\n\u001b[1;32m    857\u001b[0m     hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    859\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:336\u001b[0m, in \u001b[0;36mBartEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    333\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[1;32m    335\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m--> 336\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_fn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(hidden_states))\n\u001b[1;32m    337\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_dropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m    338\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# this array stores the 0 and 1 classifier based on if the probability is less than or greater than 0.5\n",
    "predictions = []\n",
    "# this array stores the precise probability that the model outputs\n",
    "probability_predictions = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    text = row['notes']\n",
    "    candidate_label = ['atrial fibrillation']\n",
    "\n",
    "    output = classifier(text, candidate_label)\n",
    "    # extract the exact prediction from the output, which is a dictionary with the key 'scores' storing an array\n",
    "    prediction = output['scores'][0]\n",
    "\n",
    "    probability_predictions.append(prediction)\n",
    "\n",
    "    if prediction < 0.5:\n",
    "        predictions.append(0)\n",
    "    else:\n",
    "        predictions.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n",
      "0.36363636363636365\n",
      "0.36946779515743733\n",
      "[0.14798565208911896, 0.004260016139596701, 0.007346693426370621, 0.8733732104301453, 0.47856613993644714, 0.00835508480668068, 0.30374228954315186, 0.7978691458702087, 0.3883456587791443, 0.610526442527771, 0.012758253142237663, 0.9786338210105896, 0.15835674107074738, 0.24406136572360992, 0.004112997557967901, 0.5663946270942688, 0.005485554691404104, 0.0028088341932743788, 0.452274888753891, 0.8663113713264465, 0.054318126291036606, 0.17624005675315857, 0.3799442648887634, 0.02826797217130661, 0.0018008373444899917, 0.02108229137957096, 0.8496683835983276, 0.6324237585067749, 0.005130487959831953, 0.7790483832359314, 0.9560039639472961, 0.004688859451562166, 0.7401663064956665, 0.0032245267648249865, 0.026957301422953606, 0.04533727094531059, 0.0026358540635555983, 0.9740086793899536, 0.8190281987190247, 0.021336989477276802, 0.005790151190012693, 0.00065345608163625, 0.004089518915861845, 0.0018263614038005471, 0.5236969590187073, 0.37730830907821655, 0.1969357430934906, 0.0012167893582955003, 0.0011111871572211385, 0.11721184104681015, 0.5677804350852966, 0.44589170813560486, 0.008936932310461998, 0.8350480794906616, 0.9992284774780273, 0.8039325475692749, 0.7620808482170105, 0.4740682542324066, 0.7778132557868958, 0.06792148947715759, 0.33733293414115906, 0.3692958652973175, 0.1336025446653366, 0.07248890399932861, 0.6088559031486511, 0.006421435158699751, 0.008735676296055317, 0.999435305595398, 0.8313459157943726, 0.003655131906270981, 0.4290830194950104, 0.009534123353660107, 0.30509161949157715, 0.8760485649108887, 0.011416600085794926, 0.5083655714988708, 0.8876095414161682, 0.4045895040035248, 0.6533616781234741, 0.8878891468048096, 0.5458827018737793, 0.9967803955078125, 0.16832929849624634, 0.5523111820220947, 0.9986476898193359, 0.002187735866755247, 0.09874750673770905, 0.004742438904941082, 0.0026629420462995768, 0.02677822858095169, 0.5716938376426697, 0.6904398798942566, 0.00135710125323385, 0.9979155659675598, 0.01583796553313732, 0.07795237004756927, 0.9734945893287659, 0.008443851955235004, 0.5493632555007935, 0.4749665856361389, 0.2760951817035675, 0.2349260151386261, 0.3144732415676117, 0.557910144329071, 0.004533491097390652, 0.8047829270362854, 0.24382445216178894, 0.4849304258823395, 0.2680720388889313, 0.8966773748397827, 0.0008203519391827285, 0.3518964946269989, 0.060313351452350616, 0.6086230874061584, 0.011706248857080936, 0.6875327229499817, 0.3066805303096771, 0.42276495695114136, 0.04600876569747925, 0.5558695793151855, 0.004299301188439131, 0.18941298127174377, 0.9994272589683533, 0.001729621202684939, 0.7669504880905151, 0.005431485362350941, 0.2838176488876343, 0.06431770324707031, 0.009549077600240707, 0.5371994972229004, 0.7795461416244507, 0.2670845091342926, 0.7936670184135437, 0.0012737836223095655, 0.4539976418018341, 0.004554096609354019, 0.4508633017539978, 0.002867239061743021, 0.004772427026182413, 0.0030509931966662407, 0.9983564615249634, 0.03234699368476868, 0.624403715133667, 0.908295214176178, 0.010167992673814297, 0.46520885825157166, 0.18372191488742828, 0.05445157364010811, 0.8829389214515686, 0.011386912316083908, 0.05925874412059784, 0.9958525896072388, 0.0024958716239780188, 0.8435255289077759, 0.5909790396690369, 0.10336270928382874, 0.4504043459892273, 0.7169904708862305, 0.9588450789451599, 0.0707981213927269, 0.12053417414426804, 0.9971165060997009, 0.583804190158844, 0.807233989238739, 0.28756800293922424, 0.04358530789613724, 0.015779923647642136, 0.46693989634513855, 0.4290750324726105, 0.8662894368171692, 0.8938568234443665, 0.5251797437667847, 0.032246410846710205, 0.008437605574727058, 0.085945263504982, 0.6285772323608398]\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions))\n",
    "print(np.mean(predictions))\n",
    "print(probability_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
