{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Causal Inference Synthetic Simulations \\#2\n",
    "\n",
    "In this set of experiments, we attempt to simulate text-based proximal causal inference when two pieces of text data are independent and representative of the underlying distribution of text data. Below are the steps of the experiment:\n",
    "\n",
    "1. Make a training dataset DTrain. For each row of data in DTrain, generate two realizations of X1, X2, X3, and X4. \n",
    "2. Generate X = [mean(X11, X12), mean(X21, X22), mean(X31, X32), mean(X41, X42)]. Train a single linear logistic regression on this data to predict U. This simulates training a zero-shot classifier on some broad background/training data.\n",
    "3. Make a dataset that simulates inference time DInference. For each row of data, generate two realizations of X1, X2, X3, and X4. These two realizations of X1 through X4 are independent by definition.\n",
    "4. Apply the previously trained 'zero-shot model' to obtain Z and W from the two realizations of X1, X2, X3, and X4.\n",
    "5. Proceed with proximal pipeline as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(0)\n",
    "size = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code implementing future helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_predictions(a):\n",
    "    # takes array a of probabilities, and returns a prediction for 0s and 1s\n",
    "    output = []\n",
    "\n",
    "    for ele in a:\n",
    "        if ele < 0.5:\n",
    "            output.append(0)\n",
    "        else:\n",
    "            output.append(1)\n",
    "\n",
    "    # output = np.random.binomial(1, a, len(a))\n",
    "\n",
    "    return output\n",
    "\n",
    "def proximal_find_ace(A, Y, W, Z, covariates, data):\n",
    "    # fit a model W~A+Z\n",
    "    formula = W+\"~\"+A+\"+\"+Z\n",
    "    if len(covariates) > 0:\n",
    "        formula += '+' + '+'.join(covariates)\n",
    "    model1 = sm.GLM.from_formula(formula=formula, data=data, family=sm.families.Binomial()).fit()\n",
    "\n",
    "    # make predictions for What\n",
    "    What = model1.predict(data)\n",
    "    data[\"What\"] = What\n",
    "\n",
    "    # fit a model Y~A+What\n",
    "    formula = Y+\"~\"+A+\"+What\"\n",
    "    if len(covariates) > 0:\n",
    "        formula += '+' + '+'.join(covariates)\n",
    "    model2 = sm.GLM.from_formula(formula=formula, data=data, family=sm.families.Gaussian()).fit()\n",
    "\n",
    "    # the ACE is the coefficient for A in model2\n",
    "    return model2.params[A]\n",
    "\n",
    "def compute_confidence_intervals(A, Y, W, Z, covariates, data, num_bootstraps=200, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Compute confidence intervals for backdoor adjustment via bootstrap\n",
    "    \n",
    "    Returns tuple (q_low, q_up) for the lower and upper quantiles of the confidence interval.\n",
    "    \"\"\"\n",
    "    \n",
    "    Ql = alpha/2\n",
    "    Qu = 1 - alpha/2\n",
    "    # two lists for the two indexes of output\n",
    "    estimates = []\n",
    "    \n",
    "    for i in range(num_bootstraps):\n",
    "        \n",
    "        # resample the data with replacement\n",
    "        data_sampled = data.sample(len(data), replace=True)\n",
    "        data_sampled.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # add estimate from resampled data\n",
    "        output = proximal_find_ace(A, Y, W, Z, covariates, data_sampled)\n",
    "        estimates.append(output)\n",
    "\n",
    "    # calculate the quantiles\n",
    "    quantiles = np.quantile(estimates, q=[Ql, Qu])\n",
    "    q_low = quantiles[0]\n",
    "    q_up = quantiles[1]\n",
    "    \n",
    "    return (q_low, q_up)\n",
    "\n",
    "def odds_ratio(X, Y, Z, data):\n",
    "    # calculate the odds ratio assuming linearity\n",
    "\n",
    "    formula = X + '~1+' + Y\n",
    "    if len(Z) > 0:\n",
    "        formula += '+' + '+'.join(Z)\n",
    "\n",
    "    model = sm.GLM.from_formula(formula=formula, data=data, family=sm.families.Binomial()).fit()\n",
    "\n",
    "    return np.exp(model.params[Y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the data generating process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    U = np.random.binomial(1, 0.48, size)\n",
    "\n",
    "    X11 = np.random.normal(0, 1, size) + 2*U\n",
    "    X12 = np.random.normal(0, 1, size) + 2*U\n",
    "\n",
    "    # make sure that X2 is some non-linear function\n",
    "    X21 = np.random.normal(0, 1, size) + np.exp(X11) + U\n",
    "    X22 = np.random.normal(0, 1, size) + np.exp(X12) + U\n",
    "\n",
    "    X31 = np.random.normal(0, 1, size) + 1.3*U\n",
    "    X32 = np.random.normal(0, 1, size) + 1.3*U\n",
    "\n",
    "    # make sure that X4 is some non-linear function\n",
    "    X41 = np.random.normal(0, 1, size) + X31**2 + 0.5*X31**3 + U\n",
    "    X42 = np.random.normal(0, 1, size) + X32**2 + 0.5*X32**3 + U\n",
    "\n",
    "    A = np.random.binomial(1, expit(0.8*U), size)\n",
    "\n",
    "    Y = np.random.normal(0, 1, size) + 1.3*A + 1.4*U\n",
    "\n",
    "    data = pd.DataFrame({\"U\": U, \"X11\": X11, \"X21\": X21, \"X31\": X31, \"X41\": X41, \"X12\": X12, \"X22\": X22, \"X32\": X32, \"X42\": X42,\n",
    "                         \"A\": A, \"Y\": Y})\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtrain = generate_data()\n",
    "\n",
    "# simulate the creation of a 'broad zero-shot predictor'\n",
    "Dtrain['X1'] = (Dtrain['X11']+Dtrain['X12'])/2\n",
    "Dtrain['X2'] = (Dtrain['X21']+Dtrain['X22'])/2\n",
    "Dtrain['X3'] = (Dtrain['X31']+Dtrain['X32'])/2\n",
    "Dtrain['X4'] = (Dtrain['X41']+Dtrain['X42'])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a linear logistic regressor to simulate a zero-shot classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>U</td>        <th>  No. Observations:  </th>  <td> 20000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td> 19995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>     4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>         <td>Logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -2120.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 27 Dec 2023</td> <th>  Deviance:          </th> <td>  4240.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>11:45:32</td>     <th>  Pearson chi2:      </th> <td>3.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>          <td>9</td>        <th>  Pseudo R-squ. (CS):</th>  <td>0.6905</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -6.3196</td> <td>    0.124</td> <td>  -50.834</td> <td> 0.000</td> <td>   -6.563</td> <td>   -6.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X1</th>        <td>    2.5527</td> <td>    0.125</td> <td>   20.386</td> <td> 0.000</td> <td>    2.307</td> <td>    2.798</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X2</th>        <td>    0.4191</td> <td>    0.030</td> <td>   13.920</td> <td> 0.000</td> <td>    0.360</td> <td>    0.478</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X3</th>        <td>    1.8019</td> <td>    0.092</td> <td>   19.543</td> <td> 0.000</td> <td>    1.621</td> <td>    1.983</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X4</th>        <td>    0.2225</td> <td>    0.021</td> <td>   10.645</td> <td> 0.000</td> <td>    0.182</td> <td>    0.264</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}   &        U         & \\textbf{  No. Observations:  } &    20000    \\\\\n",
       "\\textbf{Model:}           &       GLM        & \\textbf{  Df Residuals:      } &    19995    \\\\\n",
       "\\textbf{Model Family:}    &     Binomial     & \\textbf{  Df Model:          } &        4    \\\\\n",
       "\\textbf{Link Function:}   &      Logit       & \\textbf{  Scale:             } &    1.0000   \\\\\n",
       "\\textbf{Method:}          &       IRLS       & \\textbf{  Log-Likelihood:    } &   -2120.4   \\\\\n",
       "\\textbf{Date:}            & Wed, 27 Dec 2023 & \\textbf{  Deviance:          } &    4240.7   \\\\\n",
       "\\textbf{Time:}            &     11:45:32     & \\textbf{  Pearson chi2:      } &  3.86e+04   \\\\\n",
       "\\textbf{No. Iterations:}  &        9         & \\textbf{  Pseudo R-squ. (CS):} &   0.6905    \\\\\n",
       "\\textbf{Covariance Type:} &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &      -6.3196  &        0.124     &   -50.834  &         0.000        &       -6.563    &       -6.076     \\\\\n",
       "\\textbf{X1}        &       2.5527  &        0.125     &    20.386  &         0.000        &        2.307    &        2.798     \\\\\n",
       "\\textbf{X2}        &       0.4191  &        0.030     &    13.920  &         0.000        &        0.360    &        0.478     \\\\\n",
       "\\textbf{X3}        &       1.8019  &        0.092     &    19.543  &         0.000        &        1.621    &        1.983     \\\\\n",
       "\\textbf{X4}        &       0.2225  &        0.021     &    10.645  &         0.000        &        0.182    &        0.264     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Generalized Linear Model Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:                      U   No. Observations:                20000\n",
       "Model:                            GLM   Df Residuals:                    19995\n",
       "Model Family:                Binomial   Df Model:                            4\n",
       "Link Function:                  Logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -2120.4\n",
       "Date:                Wed, 27 Dec 2023   Deviance:                       4240.7\n",
       "Time:                        11:45:32   Pearson chi2:                 3.86e+04\n",
       "No. Iterations:                     9   Pseudo R-squ. (CS):             0.6905\n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -6.3196      0.124    -50.834      0.000      -6.563      -6.076\n",
       "X1             2.5527      0.125     20.386      0.000       2.307       2.798\n",
       "X2             0.4191      0.030     13.920      0.000       0.360       0.478\n",
       "X3             1.8019      0.092     19.543      0.000       1.621       1.983\n",
       "X4             0.2225      0.021     10.645      0.000       0.182       0.264\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_model = sm.GLM.from_formula(formula=\"U~X1+X2+X3+X4\", data=Dtrain, family=sm.families.Binomial()).fit()\n",
    "\n",
    "zero_shot_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the inference dataset, and generate Z and W by using the two realizations of the covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DInference = generate_data()\n",
    "\n",
    "# create the two realizations of the covariates to generate Z and W\n",
    "realization1 = pd.DataFrame()\n",
    "realization1['X1'] = DInference['X11']\n",
    "realization1['X2'] = DInference['X21']\n",
    "realization1['X3'] = DInference['X31']\n",
    "realization1['X4'] = DInference['X41']\n",
    "\n",
    "Z = binary_predictions(zero_shot_model.predict(realization1))\n",
    "\n",
    "realization2 = pd.DataFrame()\n",
    "realization2['X1'] = DInference['X12']\n",
    "realization2['X2'] = DInference['X22']\n",
    "realization2['X3'] = DInference['X32']\n",
    "realization2['X4'] = DInference['X42']\n",
    "\n",
    "W = binary_predictions(zero_shot_model.predict(realization2))\n",
    "\n",
    "DInference['Z'] = Z\n",
    "DInference['W'] = W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify odds ratio values between W and Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odds ratio: 18.79333254138656\n",
      "odds ratio, condition U: 1.093885938729984\n"
     ]
    }
   ],
   "source": [
    "print(\"odds ratio:\", odds_ratio(\"W\", \"Z\", [], DInference))\n",
    "print(\"odds ratio, condition U:\", odds_ratio(\"W\", \"Z\", [\"U\"], DInference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceed with the causal inference pipeline as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3094915115120327\n",
      "(1.2766741073004277, 1.3417188128672284)\n"
     ]
    }
   ],
   "source": [
    "print(proximal_find_ace(\"A\", \"Y\", \"W\", \"Z\", [], DInference))\n",
    "print(compute_confidence_intervals(\"A\", \"Y\", \"W\", \"Z\", [], DInference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, using the same model to generate Z and W creates proxies that are valid for proximal causal inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
